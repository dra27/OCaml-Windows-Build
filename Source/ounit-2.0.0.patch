diff -Naur ounit-2.0.0/doc/manual.txt ounit-2.0.0/doc/manual.txt
--- ounit-2.0.0/doc/manual.txt	2013-09-30 02:22:18.000000000 +0200
+++ ounit-2.0.0/doc/manual.txt	2014-04-05 11:25:50.397162500 +0200
@@ -1,20 +1,19 @@
 {!indexlist}
 
-{2 What is unit Testing?}
+{2 What is Unit Testing?}
 
-A test-oriented methodology for software development is most
-effective whent tests are easy to create, change, and execute. The
-JUnit tool pioneerded for test-first development in Java. OUnit is
-an adaptation of JUnit to OCaml.
-
-With OUnit, as with JUnit, you can easily create tests, name them,
-group them into suites, and execute them, with the framework
-checking the results automatically.
+A test-oriented methodology for software development is most effective when
+tests are easy to create, change, and execute. The JUnit tool pioneered
+test-first development in Java; OUnit is an adaptation of JUnit for OCaml.
+
+With OUnit, as with JUnit, one can easily create tests, name them, group them
+into suites, and execute them, with the framework automatically checking the
+results.
 
 {2 Getting Started}
 
 The basic principle of a test suite is to have a file {i test.ml} which will
-contain the tests, and an OCaml module under test, named {i foo.ml}.
+contain the tests, and an OCaml module under inspection named {i foo.ml}.
 
 File {i foo.ml}:
 {[
@@ -24,18 +23,18 @@
 let fgeneric () = failwith "Not implemented";;
 ]}
 
-The main point of a test is to check that the function under test has the
-expected behavior. You check the behavior using assert functions. The most
-simple one is {!OUnit2.assert_equal}. This function compares the result of the
-function with an expected result.
+The main point of a test is to check that the function being tested has the
+expected behavior. The behavior is checked using assert functions. The simplest
+is {!OUnit2.assert_equal}, which compares the result of the function with an
+expected result.
 
 The most useful functions are:
-- {!OUnit2.assert_equal} the basic assert function
-- {!OUnit2.(>:::)} to define a list of tests
-- {!OUnit2.(>::)} to name a test
-- {!OUnit2.run_test_tt_main} to run the test suite you define
-- {!OUnit2.bracket_tmpfile} that create a temporary filename.
-- {!OUnit2.bracket_tmpdir} that create a temporary directory.
+- {!OUnit2.assert_equal}, the basic assert function
+- {!OUnit2.(>:::)}, to define a list of tests
+- {!OUnit2.(>::)}, to name a test
+- {!OUnit2.run_test_tt_main}, to run a test suite
+- {!OUnit2.bracket_tmpfile}, to create a temporary file
+- {!OUnit2.bracket_tmpdir}, to create a temporary directory
 
 File {i test.ml}:
 {[
@@ -63,8 +62,8 @@
 $ ocamlfind ocamlc -o test -package oUnit -linkpkg -g foo.ml test.ml
 ]}
 
-A executable named "test" will be created. When run it produces the
-following output.
+An executable named "test" will be created. When run, it produces the following
+output:
 
 {[
 $ ./tests
@@ -73,31 +72,30 @@
 OK
 ]}
 
-When using {!OUnit2.run_test_tt_main}, a non zero exit code signals that the
-test suite was not successful.
+When using {!OUnit2.run_test_tt_main}, a non-zero exit code signals that the
+test suite was not successfully executed.
 
 {2 Advanced usage}
 
-The topics, cover here, are only for advanced users who wish to unravel the
+The topics covered here are only for advanced users who wish to unravel the
 power of OUnit.
 
 {!modules: OUnit2}
 
 {3 Error reporting}
 
-The error reporting part of OUnit is quite important. If you want to identify
-the failure, you should tune the display of the value and the test.
+The error reporting part of OUnit is quite important. To help identify the
+failure, the display of the value and the test can be tuned.
 
-Here is a list of thing you can display:
-- name of the test: OUnit use numbers to define path's test. But an error
-  reporting about a failed test "0:1:2" is less explicit than
+OUnit can display:
+- the name of the test: OUnit uses numbers to define the path of the test.
+  However, an error reporting a failed test "0:1:2" is less explicit than
   "OUnit:0:comparator:1:float_comparator:2"
-- [~msg] parameter: it allows you to define say which assert has failed in your
-  test. When you have more than one assert in a test, you should provide a
-  [~msg] to be able to make the difference
-- [~printer] parameter: {!OUnit2.assert_equal} allows you to define a printer for
-  compared values. A message ["abcd" is not equal to "defg"] is better than [not
-  equal]
+- an indication of which assertion has failed in the test using the [~msg]
+  parameter
+- actual values in {!OUnit2.assert_equal} using the [~printer] parameter which
+  allows the clearer message ["abcd" is not equal to "defg"] instead of
+  [not equal]
 
 {[
 open OUnit2;;
@@ -115,16 +113,15 @@
 
 {3 Command line arguments}
 
-{!OUnit2.run_test_tt_main} already provides a set of command line argument to
-help user to run only the test he wants:
-- [-only-test]: skip all the tests except this one, you can use this flag
-  several time to select more than one test to run
+{!OUnit2.run_test_tt_main} provides a set of command line arguments to help the
+user run only the test required:
+- [-only-test]: skip all the tests except this one; can be used multiple times
+  to select multiple tests
 - [-list-test]: list all the available tests and exit
 - [-help]: display help message and exit
 
-It is also possible to add your own command-line arguments, environment
-variable and config file variable. You should do it if you want to define some
-extra arguments.
+It is also possible to add command-line arguments, environment variables and
+config. file variables.
 
 For example:
 
@@ -144,18 +141,18 @@
 ;;
 ]}
 
-The [Conf.make_*] creates a command line argument, an environment variable and
-a config file variable.
+The [Conf.make_*] creates a command line argument, an environment variable and a
+config. file variable.
 
 {3 Skip and todo tests}
 
 Tests are not always meaningful and can even fail because something is missing
-in the environment. In order to manage this, you can define a skip condition
-that will skip the test.
+in the environment. In order to manage this, a skip condition can be defined to
+omit the test under certain circumstances.
 
-If you start by defining your tests rather than implementing the functions
-under test, you know that some tests will just fail. You can mark these tests
-as to do tests, this way they will be reported differently in your test suite.
+If tests are defined before the functions themselves are implemented then some
+tests will of course fail. These tests can be marked as 'to do' tests, and will
+be reported differently in the test suite.
 
 {[
 open OUnit2;;
@@ -165,7 +162,7 @@
   [
     "funix">::
     (fun test_ctxt ->
-      skip_if (Sys.os_type = "Win32") "Don't work on Windows";
+      skip_if (Sys.os_type = "Win32") "Doesn't work on Windows";
       assert_equal
         0
         (Foo.funix ()));
@@ -182,29 +179,28 @@
 
 {3 Effective OUnit}
 
-This section is about general tips about unit testing and OUnit. It is the
-result of some years using OUnit in real world applications.
+This section is general tips about unit testing and OUnit and is the result of
+some years using OUnit in real world applications.
 
-- test everything: the more you create tests, the better chance you have to
-  catch early an error in your program. Every submitted bugs to your application
-  should have a matching tests. This is a good practice, but it is not always
-  easy to implement.
-- test only what is really exported: on the long term, you have to maintain your
-  test suite. If you test low-level functions, you'll have a lot of tests to
-  rewrite. You should focus on creating tests for functions for which the
-  behavior shouldn't change.
-- test fast: the best test suite is the one that runs after every single build.
-  You should set your default Makefile target to run the test suite. It means
-  that your test suite should be fast to run, typically, a 10s test suite is
-  fine.
-- test long: contrary to the former tip, you should also have a complete test
-  suite which can be very long to run. The best way to achieve both tips, is to
-  define a command line arguments [-long] and skip the tests that are too long in
-  your test suite according to it. When you do a release, you should use run
-  your long test suite.
-- family tests: when testing behavior, most of the time you call exactly the
-  same code with different arguments. In this case [List.map] and
-  {!OUnit2.(>:::)} are your friends. For example:
+- test everything: the more tests created, the better the chance of catching an
+  error in the program earlier. All submitted bugs in an application should have
+  corresponding tests. This is a good practice, but is not always easy to
+  implement.
+- test only what is exported: in the long term, the test suite has to be
+  maintained. If low-level functions are tested, then lots of tests may have to
+  be rewritten. Tests should focus on verifying behavior which shouldn't change.
+- test fast: the best test suite is that which can run after every single build
+  and so allow the default Makefile target for the project to run the test
+  suite. It means that the test suite needs to be quick to run: typically, a
+  10 second test suite is fine.
+- test thoroughly: equally, there should also be a complete test suite which may
+  take much longer to run. The best way to achieve both goals is to define a
+  command line argument [-long] and skip the tests which are too time-consuming
+  in the test suite if it is not specified. When making a release, the entire
+  test suite should be run.
+- test families: when testing behavior, often the same code is called multiple
+  times but with different arguments. Here, [List.map] and
+  {!OUnit2.(>:::)} come into their own. For example:
 
 {[
 open OUnit2;;
@@ -225,19 +221,19 @@
 ;;
 ]}
 
-- test failures and successes: the most obvious thing you want to test are
-  successes, i.e. that you get the expected behavior in the normal case. But
-  most of the errors arise in corner cases and in the code of the test itself.
-  For example, you can have a partial application of your {!OUnit2.assert_equal}
-  and never encounter any errors, just because the [assert_equal] is not called.
-  In this case, if you test errors as well, you will have a missing errors as
-  well.
-- set up and clean your environment in the test: you should not set up and clean
-  your test environment outside the test. Ideally, if you run no tests, the
-  program should do nothing. This is also a sane way to be sure that you are
-  always testing in a clean environment, not polluted by the result of failed
-  tests run before. This include the process environment, like current working
-  directory.
+- test failures and successes: the most obvious thing to test is successes,
+  i.e. that the code has the expected behavior in the normal case. However, most
+  of the errors arise in corner cases and in the code of the test itself. For
+  example, there may be an accidental partial application of
+  {!OUnit2.assert_equal} meaning the test never encounters any errors simply
+  because [assert_equal] is not actually called. In this case, testing errors as
+  well exposes the problem through missing errors in the test results.
+- set-up and clean the environment in the test: it should not be done outside
+  the test. Ideally, if no tests are run, the program should do nothing. This is
+  also a good paradigm for ensuring that tests are always carried out in a clean
+  environment, unpolluted by the results of (potentially failed) tests run
+  previously. This also includes the process environment, for example the
+  current working directory.
 
 {[
 open OUnit2;;
@@ -249,21 +245,23 @@
     assert_command ~chdir:"/foo/test" "ls" [])
 ;;
 ]}
-- separate your test: OUnit test code should live outside the code under a
-  directory called {i test}.  This allow to drop the dependency on OUnit when
-  distributing your library/application. This also enables people to easily
-  make a difference from what really matters (the main code) and what are only
-  tests. It is possible to have it directly in the code, like in Quickcheck
-  style tests.
-
-The unit testing scope is always hard to define. Unit testing should be about
-testing a single features. But OUnit can help you to test higher level behavior,
-by running a full program for example. While it isn't real unit testing, you
-can use OUnit to do it and should not hesitate to do it.
-
-In term of line of codes, a test suite can represent from 10% to 150% of the
-code under test. With time, your test suite will grow faster than your
-program/library. A good ratio is 33%.
+- separate the tests: OUnit test code should live separate from the main code,
+  for example in a directory called {i test}. This allows the dependency on
+  OUnit to be dropped easily when distributing the library or application and
+  also enables others to see easily the difference between the main code and
+  code which is only for tests. It is also possible to have it directly in the
+  code, as in Quickcheck (http://en.wikipedia.org/wiki/QuickCheck) style
+  testing.
+
+The unit testing scope is always hard to define. Ideally, unit testing should be
+about testing single features. However, OUnit can also help testing higher level
+behavior ('functional verification'), for example, by running a full program.
+While it isn't technically unit testing, OUnit can be effectively used for this
+too.
+
+In terms of lines of code, a test suite can represent from 10% to 150% of the
+code under test. With time, a test suite will typically grow faster than the
+library or application; a good ratio is 33%.
 
 @author Maas-Maarten Zeeman
 @author Sylvain Le Gall
